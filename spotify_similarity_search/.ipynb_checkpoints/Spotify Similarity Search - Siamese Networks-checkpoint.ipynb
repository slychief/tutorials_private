{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Songs on Spotify - Part 2 - Siamese Networks\n",
    "\n",
    "In this tutorial I will demonstrate how to apply machine learning to search for similar songs on Spotify.\n",
    "\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "1. Loading data\n",
    "2. Preprocess data\n",
    "3. Define Model\n",
    "4. Fit Model\n",
    "5. Evaluate Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requiremnts\n",
    "\n",
    "Install the following dependencies to run this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T10:20:32.488000Z",
     "start_time": "2017-08-24T10:20:32.483000Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# numeric and scientific processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import progressbar\n",
    "\n",
    "import tutorial_functions as tut_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spotipy**\n",
    "\n",
    "Spotipy is a thin client library for the Spotify Web API.\n",
    "\n",
    "https://github.com/plamere/spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import spotipy.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe how to get client credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T10:20:33.260000Z",
     "start_time": "2017-08-24T10:20:33.256000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPOTIPY_CLIENT_ID\"]     = \"8a7fffc37b6c44e6b7bc344c3295034c\"\n",
    "os.environ[\"SPOTIPY_CLIENT_SECRET\"] = \"f19dd914ba58408c9407dd6479b23812\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the following message:\n",
    "\n",
    "    User authentication requires interaction with your\n",
    "    web browser. Once you enter your credentials and\n",
    "    give authorization, you will be redirected to\n",
    "    a url.  Paste that url you were directed to to\n",
    "    complete the authorization.\n",
    "\n",
    "    Opened https://accounts.spotify.com/authorize?scope=playlist-modify-public&redirect_uri=ht...\n",
    "    \n",
    "\n",
    "You need to authenticate your browser session. Follow the link and log in to Spotify. After login, you will be redirected to http://localhost/?code=... Copy the entire URL and paste it to the prompted textbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T10:20:54.045000Z",
     "start_time": "2017-08-24T10:20:34.076000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = util.prompt_for_user_token(\"slychief\", \n",
    "                                   \"playlist-modify-public\", \n",
    "                                   redirect_uri=\"http://localhost/\")\n",
    "\n",
    "sp = spotipy.Spotify(auth=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Before we can train our models we first have to get some data.\n",
    "\n",
    "## Download Echonest Features from Spotify\n",
    "\n",
    "We use spotipy to access the Spotify API and to download metadata and audio features of Spotify tracks. The following list provides a selection of Spotify playlists of various music genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlists = [\n",
    "    \n",
    "     {\"name\": \"clubbeats\",    \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DXbX3zSzB4MO0\"},\n",
    "     {\"name\": \"softpop\",      \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DWTwnEm1IYyoj\"},\n",
    "     {\"name\": \"electropop\",   \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DX4uPi2roRUwU\"},\n",
    "     {\"name\": \"rockclassics\", \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DWXRqgorJj26U\"},\n",
    "     {\"name\": \"rockhymns\",    \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DX4vth7idTQch\"},\n",
    "     {\"name\": \"soft_rock\",    \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DX6xOPeSOGone\"},\n",
    "     {\"name\": \"metalcore\",    \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DWXIcbzpLauPS\"}, \n",
    "     {\"name\": \"metal\",        \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DWWOaP4H0w5b0\"},\n",
    "     {\"name\": \"classic_metal\",\"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DX2LTcinqsO68\"},\n",
    "     {\"name\": \"grunge\",       \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DX11ghcIxjcjE\"},\n",
    "     {\"name\": \"hiphop\",       \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DWVdgXTbYm2r0\"},\n",
    "     {\"name\": \"poppunk\",      \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DXa9wYJr1oMFq\"},\n",
    "     {\"name\": \"classic\",      \"uri\": \"spotify:user:spotify:playlist:37i9dQZF1DXcN1fAVSf7CR\"}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching with joblib\n",
    "\n",
    "We will use caching to locally store retrieved data. This is on the one hand a requirement of the API and on the other it speeds up processing when we reload the notebook. *joblib* is a convenient library which simplifies caching.\n",
    "\n",
    "*Update the cachdir to an appropriate path in the following cell*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T10:20:32.875000Z",
     "start_time": "2017-08-24T10:20:32.870000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "memory = Memory(cachedir='/home/schindler/tmp/spotify/', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_spotify_data(track_id):\n",
    "    \n",
    "    # meta-data\n",
    "    track_metadata      = sp.track(track_id)\n",
    "    album_metadata      = sp.album(track_metadata[\"album\"][\"id\"])\n",
    "    artist_metadata     = sp.artist(track_metadata[\"artists\"][0][\"id\"])\n",
    "    \n",
    "    # feature-data\n",
    "    sequential_features = sp.audio_analysis(track_id)\n",
    "    trackbased_features = sp.audio_features([track_id])\n",
    "    \n",
    "    return track_metadata, album_metadata, artist_metadata, sequential_features, trackbased_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62% (606 of 970) |#################################################################################################################                                                                     | Elapsed Time: 0:00:25 ETA: 0:00:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 65530 (char 65529)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% (942 of 970) |################################################################################################################################################################################      | Elapsed Time: 0:00:40 ETA: 0:00:01"
     ]
    }
   ],
   "source": [
    "# Get Playlist meta-data\n",
    "playlists = tut_func.get_playlist_metadata(sp, playlists)\n",
    "\n",
    "# Get track-ids of all playlist entries\n",
    "playlists = tut_func.get_track_ids(sp, playlists)\n",
    "\n",
    "num_tracks_total = np.sum([playlist[\"num_tracks\"] for playlist in playlists])\n",
    "\n",
    "# Fetch data and features from Spotify\n",
    "pbar = progressbar.ProgressBar(max_value=num_tracks_total)\n",
    "pbar.start()\n",
    "\n",
    "raw_track_data      = []\n",
    "processed_track_ids = []\n",
    "\n",
    "for playlist in playlists:\n",
    "\n",
    "    for track_id in playlist[\"track_ids\"]:\n",
    "\n",
    "        try:\n",
    "            # avoid duplicates in the data-set\n",
    "            if track_id not in processed_track_ids:\n",
    "\n",
    "                # retrieve data from Spotify\n",
    "                spotify_data = get_spotify_data(track_id)\n",
    "\n",
    "                raw_track_data.append([playlist[\"name\"], spotify_data])\n",
    "                processed_track_ids.append(track_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print e\n",
    "\n",
    "        pbar.update(len(raw_track_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data\n",
    "\n",
    "Currently we only have a list of raw data-objects retrieved from the Spotify API. We need to transform this information to a more structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Meta-data\n",
    "metadata = tut_func.aggregate_metadata(raw_track_data)\n",
    "\n",
    "# Aggregate Feature-data\n",
    "feature_data = tut_func.aggregate_featuredata(raw_track_data, metadata)\n",
    "\n",
    "# standardize sequential_features\n",
    "feature_data -= feature_data.mean(axis=0)\n",
    "feature_data /= feature_data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(feature_data, metadata, num_pairs_per_track):\n",
    "    \n",
    "    data_pairs = []\n",
    "    labels     = []\n",
    "    \n",
    "    for row_id, q_track in metadata.sample(frac=1).iterrows():\n",
    "        \n",
    "        for _ in range(num_pairs_per_track):\n",
    "            \n",
    "            # search similar and dissimilar examples\n",
    "            pos_example = metadata[metadata.playlist == q_track.playlist].sample(1)\n",
    "            neg_example = metadata[metadata.playlist != q_track.playlist].sample(1)\n",
    "\n",
    "            # create feature pairs\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[pos_example.index]][0]])\n",
    "            labels.append(1)\n",
    "\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[neg_example.index]][0]])\n",
    "            labels.append(0)\n",
    "\n",
    "    return np.array(data_pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18880, 2, 69)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pairs, labels = create_pairs(feature_data, metadata, 10)\n",
    "\n",
    "data_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Nadam, SGD\n",
    "from keras.regularizers import l2, l1\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_network(input_dim):\n",
    "\n",
    "    input_left  = Input(shape=input_dim)\n",
    "    input_right = Input(shape=input_dim)\n",
    "\n",
    "    layers_left  = Dense(100, activation=\"selu\")(input_left)\n",
    "    layers_left  = Dense(100, activation=\"selu\")(layers_left)\n",
    "    layers_left  = Dense(100, activation=\"selu\")(layers_left)\n",
    "    layers_right = Dense(100, activation=\"selu\")(input_right)\n",
    "    layers_right = Dense(100, activation=\"selu\")(layers_right)\n",
    "    layers_right = Dense(100, activation=\"selu\")(layers_right)\n",
    "\n",
    "    L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "\n",
    "    distance = Lambda(L1_distance,\n",
    "                      output_shape=lambda x: x[0])([layers_left, layers_right])\n",
    "\n",
    "    prediction = Dense(100, activation=\"elu\")(distance)\n",
    "\n",
    "    prediction = Dense(1, activation=\"sigmoid\")(prediction)\n",
    "\n",
    "    model = Model([input_left, input_right], prediction)\n",
    "\n",
    "    # train\n",
    "    rms = Nadam(lr=0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=rms, metrics=[\"mean_squared_error\", \"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_siamese_network(data_pairs[:,0].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.1481 - mean_squared_error: 0.1481 - acc: 0.7882     \n",
      "Epoch 2/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.1038 - mean_squared_error: 0.1038 - acc: 0.8584     \n",
      "Epoch 3/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0870 - mean_squared_error: 0.0870 - acc: 0.8837     \n",
      "Epoch 4/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0744 - mean_squared_error: 0.0744 - acc: 0.9015     \n",
      "Epoch 5/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0646 - mean_squared_error: 0.0646 - acc: 0.9163     \n",
      "Epoch 6/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0582 - mean_squared_error: 0.0582 - acc: 0.9236     \n",
      "Epoch 7/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0512 - mean_squared_error: 0.0512 - acc: 0.9323     \n",
      "Epoch 8/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0462 - mean_squared_error: 0.0462 - acc: 0.9404     \n",
      "Epoch 9/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0401 - mean_squared_error: 0.0401 - acc: 0.9489     \n",
      "Epoch 10/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0370 - mean_squared_error: 0.0370 - acc: 0.9524     \n",
      "Epoch 11/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0347 - mean_squared_error: 0.0347 - acc: 0.9558     \n",
      "Epoch 12/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0315 - mean_squared_error: 0.0315 - acc: 0.9598     \n",
      "Epoch 13/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0315 - mean_squared_error: 0.0315 - acc: 0.9602     \n",
      "Epoch 14/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0262 - mean_squared_error: 0.0262 - acc: 0.9680     \n",
      "Epoch 15/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0239 - mean_squared_error: 0.0239 - acc: 0.9700     \n",
      "Epoch 16/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0225 - mean_squared_error: 0.0225 - acc: 0.9711     \n",
      "Epoch 17/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0223 - mean_squared_error: 0.0223 - acc: 0.9720     \n",
      "Epoch 18/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0183 - mean_squared_error: 0.0183 - acc: 0.9779     \n",
      "Epoch 19/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0201 - mean_squared_error: 0.0201 - acc: 0.9745     \n",
      "Epoch 20/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0201 - mean_squared_error: 0.0201 - acc: 0.9757     \n",
      "Epoch 21/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0195 - mean_squared_error: 0.0195 - acc: 0.9756     \n",
      "Epoch 22/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0170 - mean_squared_error: 0.0170 - acc: 0.9782     \n",
      "Epoch 23/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0174 - mean_squared_error: 0.0174 - acc: 0.9776      ETA: 0s - loss: 0.0107 - mean_squared\n",
      "Epoch 24/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0160 - mean_squared_error: 0.0160 - acc: 0.9792     \n",
      "Epoch 25/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0154 - mean_squared_error: 0.0154 - acc: 0.9810     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faaeb01f950>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_pairs[:, 0], data_pairs[:, 1]], labels, batch_size=24, verbose=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar(query_idx):\n",
    "    res = [model.predict([feature_data[[query_idx]], feature_data[[i]]]) for i in range(feature_data.shape[0])]\n",
    "\n",
    "    res = np.array(res)\n",
    "    res = res.reshape(res.shape[0])\n",
    "\n",
    "    si = np.argsort(res)[::-1]\n",
    "\n",
    "    display_cols = [\"artist_name\", \"title\", \"album_name\", \"year\", \"playlist\"]\n",
    "    \n",
    "    print metadata.iloc[query_idx]\n",
    "\n",
    "    return metadata.loc[si, display_cols][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id                                  4CJVkjo5WpmUAKp3R44LNb\n",
      "artist_name                                       Lynyrd Skynyrd\n",
      "title                                         Sweet Home Alabama\n",
      "album_name                                        Second Helping\n",
      "label                   Digital Distribution Trinidad and Tobago\n",
      "duration                                                  281147\n",
      "popularity                                                    77\n",
      "year                                                        1974\n",
      "genres         [album rock, blues-rock, classic rock, country...\n",
      "playlist                                            rockclassics\n",
      "Name: 381, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>album_name</th>\n",
       "      <th>year</th>\n",
       "      <th>playlist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Counting Crows</td>\n",
       "      <td>Big Yellow Taxi</td>\n",
       "      <td>Films About Ghosts (The Best Of Counting Crows...</td>\n",
       "      <td>2003</td>\n",
       "      <td>soft_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Creedence Clearwater Revival</td>\n",
       "      <td>Down On The Corner</td>\n",
       "      <td>Willy And The Poor Boys (40th Anniversary Edit...</td>\n",
       "      <td>1969</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Bob Seger</td>\n",
       "      <td>Ramblin' Gamblin' Man</td>\n",
       "      <td>Ramblin' Gamblin' Man</td>\n",
       "      <td>1969</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>The Rolling Stones</td>\n",
       "      <td>Brown Sugar - Remastered</td>\n",
       "      <td>Sticky Fingers (Deluxe)</td>\n",
       "      <td>1971</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>AC/DC</td>\n",
       "      <td>Back In Black</td>\n",
       "      <td>Back In Black</td>\n",
       "      <td>1980</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Lynyrd Skynyrd</td>\n",
       "      <td>Gimme Three Steps</td>\n",
       "      <td>(Pronounced 'Leh-'Nérd 'Skin-'Nérd) [Expanded ...</td>\n",
       "      <td>1973</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Steppenwolf</td>\n",
       "      <td>Magic Carpet Ride</td>\n",
       "      <td>Born To Be Wild: A Retrospective</td>\n",
       "      <td>1991</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Keane</td>\n",
       "      <td>Everybody's Changing</td>\n",
       "      <td>Hopes and Fears (Deluxe Edition)</td>\n",
       "      <td>2009</td>\n",
       "      <td>soft_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Life In The Fast Lane</td>\n",
       "      <td>Hotel California (Remastered)</td>\n",
       "      <td>1976</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Aerosmith</td>\n",
       "      <td>Dream On</td>\n",
       "      <td>Aerosmith</td>\n",
       "      <td>1973</td>\n",
       "      <td>rockclassics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      artist_name                     title  \\\n",
       "549                Counting Crows           Big Yellow Taxi   \n",
       "328  Creedence Clearwater Revival        Down On The Corner   \n",
       "267                     Bob Seger     Ramblin' Gamblin' Man   \n",
       "323            The Rolling Stones  Brown Sugar - Remastered   \n",
       "270                         AC/DC             Back In Black   \n",
       "339                Lynyrd Skynyrd         Gimme Three Steps   \n",
       "367                   Steppenwolf         Magic Carpet Ride   \n",
       "546                         Keane      Everybody's Changing   \n",
       "277                        Eagles     Life In The Fast Lane   \n",
       "383                     Aerosmith                  Dream On   \n",
       "\n",
       "                                            album_name  year      playlist  \n",
       "549  Films About Ghosts (The Best Of Counting Crows...  2003     soft_rock  \n",
       "328  Willy And The Poor Boys (40th Anniversary Edit...  1969  rockclassics  \n",
       "267                              Ramblin' Gamblin' Man  1969  rockclassics  \n",
       "323                            Sticky Fingers (Deluxe)  1971  rockclassics  \n",
       "270                                      Back In Black  1980  rockclassics  \n",
       "339  (Pronounced 'Leh-'Nérd 'Skin-'Nérd) [Expanded ...  1973  rockclassics  \n",
       "367                   Born To Be Wild: A Retrospective  1991  rockclassics  \n",
       "546                   Hopes and Fears (Deluxe Edition)  2009     soft_rock  \n",
       "277                      Hotel California (Remastered)  1976  rockclassics  \n",
       "383                                          Aerosmith  1973  rockclassics  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(similarity_function, cut_off):\n",
    "\n",
    "    global dist\n",
    "    \n",
    "    all_precisions = []\n",
    "\n",
    "    for idx in metadata.index.values:\n",
    "\n",
    "        dist           = similarity_function(feature_data, feature_data[[idx]])\n",
    "        dist           = np.array(dist).reshape(len(dist))\n",
    "        similar_tracks = metadata.loc[np.argsort(dist)[::-1][:cut_off]]\n",
    "        same_label     = similar_tracks[\"playlist\"] == metadata.loc[idx, \"playlist\"]\n",
    "        precision      = same_label.sum() / float(cut_off)\n",
    "        all_precisions.append(precision)\n",
    "\n",
    "    all_precisions = np.array(all_precisions)\n",
    "\n",
    "    return all_precisions.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82388771186440679"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lambda x,y: [model.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlist_names = [pl[\"name\"] for pl in playlists]\n",
    "\n",
    "playlist_similarities = pd.DataFrame(np.zeros((len(playlist_names),len(playlist_names))), \n",
    "                                     index   = playlist_names, \n",
    "                                     columns = playlist_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clubbeats</th>\n",
       "      <th>softpop</th>\n",
       "      <th>electropop</th>\n",
       "      <th>rockclassics</th>\n",
       "      <th>rockhymns</th>\n",
       "      <th>soft_rock</th>\n",
       "      <th>metalcore</th>\n",
       "      <th>metal</th>\n",
       "      <th>classic_metal</th>\n",
       "      <th>grunge</th>\n",
       "      <th>hiphop</th>\n",
       "      <th>poppunk</th>\n",
       "      <th>classic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clubbeats</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>softpop</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electropop</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rockclassics</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rockhymns</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft_rock</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metalcore</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metal</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classic_metal</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grunge</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiphop</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poppunk</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               clubbeats  softpop  electropop  rockclassics  rockhymns  \\\n",
       "clubbeats            1.0      0.4         0.8           0.0        0.0   \n",
       "softpop              0.4      1.0         0.4           0.0        0.0   \n",
       "electropop           0.8      0.4         1.0           0.0        0.0   \n",
       "rockclassics         0.0      0.0         0.0           1.0        0.7   \n",
       "rockhymns            0.0      0.0         0.0           0.7        1.0   \n",
       "soft_rock            0.0      0.2         0.0           0.3        0.3   \n",
       "metalcore            0.0      0.0         0.0           0.0        0.0   \n",
       "metal                0.0      0.0         0.0           0.0        0.0   \n",
       "classic_metal        0.0      0.0         0.0           0.0        0.0   \n",
       "grunge               0.0      0.0         0.0           0.0        0.2   \n",
       "hiphop               0.0      0.1         0.4           0.0        0.0   \n",
       "poppunk              0.0      0.0         0.0           0.4        0.5   \n",
       "classic              0.0      0.0         0.0           0.0        0.0   \n",
       "\n",
       "               soft_rock  metalcore  metal  classic_metal  grunge  hiphop  \\\n",
       "clubbeats            0.0        0.0    0.0            0.0     0.0     0.0   \n",
       "softpop              0.2        0.0    0.0            0.0     0.0     0.1   \n",
       "electropop           0.0        0.0    0.0            0.0     0.0     0.4   \n",
       "rockclassics         0.3        0.0    0.0            0.0     0.0     0.0   \n",
       "rockhymns            0.3        0.0    0.0            0.0     0.2     0.0   \n",
       "soft_rock            1.0        0.0    0.0            0.0     0.0     0.0   \n",
       "metalcore            0.0        1.0    0.7            0.6     0.0     0.0   \n",
       "metal                0.0        0.7    1.0            0.8     0.5     0.0   \n",
       "classic_metal        0.0        0.6    0.8            1.0     0.5     0.0   \n",
       "grunge               0.0        0.0    0.5            0.5     1.0     0.0   \n",
       "hiphop               0.0        0.0    0.0            0.0     0.0     1.0   \n",
       "poppunk              0.0        0.0    0.6            0.4     0.0     0.0   \n",
       "classic              0.0        0.0    0.0            0.0     0.0     0.0   \n",
       "\n",
       "               poppunk  classic  \n",
       "clubbeats          0.0      0.0  \n",
       "softpop            0.0      0.0  \n",
       "electropop         0.0      0.0  \n",
       "rockclassics       0.4      0.0  \n",
       "rockhymns          0.5      0.0  \n",
       "soft_rock          0.0      0.0  \n",
       "metalcore          0.0      0.0  \n",
       "metal              0.6      0.0  \n",
       "classic_metal      0.4      0.0  \n",
       "grunge             0.0      0.0  \n",
       "hiphop             0.0      0.0  \n",
       "poppunk            1.0      0.0  \n",
       "classic            0.0      1.0  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = [[[\"clubbeats\",    \"electropop\"],    0.8],\n",
    "       [[\"clubbeats\",    \"softpop\"],      0.4],\n",
    "       [[\"electropop\",   \"hiphop\"],      0.4],\n",
    "       [[\"softpop\",      \"soft_rock\"],      0.2],\n",
    "       [[\"softpop\",      \"electropop\"],      0.4],\n",
    "       [[\"softpop\",    \"hiphop\"],      0.1],\n",
    "       [[\"rockclassics\",    \"rockhymns\"],      0.7],\n",
    "       [[\"soft_rock\",    \"rockclassics\"],      0.3],\n",
    "       [[\"soft_rock\",    \"rockhymns\"],      0.3],\n",
    "       [[\"metalcore\",    \"metal\"],      0.7],\n",
    "       [[\"metalcore\",    \"classic_metal\"],      0.6],\n",
    "       [[\"metal\",    \"classic_metal\"],      0.8],\n",
    "       [[\"classic_metal\",    \"grunge\"],      0.5],\n",
    "       [[\"metal\",    \"grunge\"],      0.5],\n",
    "       [[\"rockhymns\",    \"grunge\"],      0.2],\n",
    "       [[\"poppunk\",    \"metal\"],      0.6],\n",
    "       [[\"poppunk\",    \"classic_metal\"],      0.4],\n",
    "       [[\"poppunk\",    \"rockhymns\"],      0.5],\n",
    "       [[\"poppunk\",    \"rockclassics\"],      0.4]\n",
    "     ]\n",
    "\n",
    "# self-similarity\n",
    "for i in range(len(playlist_names)):\n",
    "    for j in range(len(playlist_names)):\n",
    "        if i == j:\n",
    "            playlist_similarities.iloc[i,j] = 1.0\n",
    "\n",
    "for s in sim:\n",
    "    playlist_similarities.loc[s[0][0],s[0][1]] = s[1]\n",
    "    playlist_similarities.loc[s[0][1],s[0][0]] = s[1]\n",
    "\n",
    "playlist_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pairs_with_sims(feature_data, metadata, num_pairs_per_track, playlist_similarities):\n",
    "    \n",
    "    data_pairs = []\n",
    "    labels     = []\n",
    "    \n",
    "    for row_id, q_track in metadata.sample(frac=1).iterrows():\n",
    "        \n",
    "        for _ in range(num_pairs_per_track):\n",
    "            \n",
    "            # search similar and dissimilar examples\n",
    "            pos_example = metadata[metadata.playlist == q_track.playlist].sample(1)\n",
    "            neg_example = metadata[metadata.playlist != q_track.playlist].sample(1)\n",
    "\n",
    "            # create feature pairs\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[pos_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, pos_example.playlist])\n",
    "\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[neg_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, neg_example.playlist])\n",
    "\n",
    "    return np.array(data_pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs, labels = create_pairs_with_sims(feature_data, metadata, 10, playlist_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18880, 2, 69)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_siamese_network(data_pairs[:,0].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.1089 - mean_squared_error: 0.1089 - acc: 0.7477     \n",
      "Epoch 2/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0701 - mean_squared_error: 0.0701 - acc: 0.8034     \n",
      "Epoch 3/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0566 - mean_squared_error: 0.0566 - acc: 0.8189     \n",
      "Epoch 4/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0476 - mean_squared_error: 0.0476 - acc: 0.8310     \n",
      "Epoch 5/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0399 - mean_squared_error: 0.0399 - acc: 0.8417     \n",
      "Epoch 6/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0347 - mean_squared_error: 0.0347 - acc: 0.8470     \n",
      "Epoch 7/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0300 - mean_squared_error: 0.0300 - acc: 0.8524     \n",
      "Epoch 8/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0270 - mean_squared_error: 0.0270 - acc: 0.8544     \n",
      "Epoch 9/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0244 - mean_squared_error: 0.0244 - acc: 0.8571     \n",
      "Epoch 10/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0220 - mean_squared_error: 0.0220 - acc: 0.8589     \n",
      "Epoch 11/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0194 - mean_squared_error: 0.0194 - acc: 0.8612     \n",
      "Epoch 12/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0192 - mean_squared_error: 0.0192 - acc: 0.8611     \n",
      "Epoch 13/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0169 - mean_squared_error: 0.0169 - acc: 0.8634     \n",
      "Epoch 14/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0145 - mean_squared_error: 0.0145 - acc: 0.8654     \n",
      "Epoch 15/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0141 - mean_squared_error: 0.0141 - acc: 0.8660     \n",
      "Epoch 16/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0135 - mean_squared_error: 0.0135 - acc: 0.8659     \n",
      "Epoch 17/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0125 - mean_squared_error: 0.0125 - acc: 0.8664     \n",
      "Epoch 18/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0108 - mean_squared_error: 0.0108 - acc: 0.8680     \n",
      "Epoch 19/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0120 - mean_squared_error: 0.0120 - acc: 0.8659     \n",
      "Epoch 20/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0105 - mean_squared_error: 0.0105 - acc: 0.8675     \n",
      "Epoch 21/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0094 - mean_squared_error: 0.0094 - acc: 0.8681     \n",
      "Epoch 22/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0096 - mean_squared_error: 0.0096 - acc: 0.8680     \n",
      "Epoch 23/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0088 - mean_squared_error: 0.0088 - acc: 0.8682     \n",
      "Epoch 24/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0073 - mean_squared_error: 0.0073 - acc: 0.8693     \n",
      "Epoch 25/25\n",
      "18880/18880 [==============================] - 0s - loss: 0.0078 - mean_squared_error: 0.0078 - acc: 0.8689     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faad9dfd4d0>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_pairs[:, 0], data_pairs[:, 1]], labels, batch_size=24, verbose=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83548728813559325"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lambda x,y: [model.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pairs_with_sims_and_identity(feature_data, metadata, num_pairs_per_track, playlist_similarities):\n",
    "    \n",
    "    data_pairs = []\n",
    "    labels     = []\n",
    "    \n",
    "    for row_id, q_track in metadata.sample(frac=1).iterrows():\n",
    "        \n",
    "        data_pairs.append([feature_data[[row_id]][0], feature_data[[row_id]][0]])\n",
    "        labels.append(1)\n",
    "        \n",
    "        for _ in range(num_pairs_per_track):\n",
    "            \n",
    "            # search similar and dissimilar examples\n",
    "            pos_example = metadata[metadata.playlist == q_track.playlist].sample(1)\n",
    "            neg_example = metadata[metadata.playlist != q_track.playlist].sample(1)\n",
    "\n",
    "            # create feature pairs\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[pos_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, pos_example.playlist] - 0.1)\n",
    "\n",
    "            data_pairs.append([feature_data[[row_id]][0], feature_data[[neg_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, neg_example.playlist] - 0.1)\n",
    "\n",
    "    return np.array(data_pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs, labels = create_pairs_with_sims_and_identity(feature_data, metadata, 10, playlist_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_siamese_network(data_pairs[:,0].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.1081 - mean_squared_error: 0.1081 - acc: 0.0517     \n",
      "Epoch 2/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0698 - mean_squared_error: 0.0698 - acc: 0.0551     \n",
      "Epoch 3/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0567 - mean_squared_error: 0.0567 - acc: 0.0564     \n",
      "Epoch 4/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0467 - mean_squared_error: 0.0467 - acc: 0.0570     \n",
      "Epoch 5/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0398 - mean_squared_error: 0.0398 - acc: 0.0572     \n",
      "Epoch 6/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0342 - mean_squared_error: 0.0342 - acc: 0.0575     \n",
      "Epoch 7/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0299 - mean_squared_error: 0.0299 - acc: 0.0578     \n",
      "Epoch 8/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0259 - mean_squared_error: 0.0259 - acc: 0.0578     \n",
      "Epoch 9/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0232 - mean_squared_error: 0.0232 - acc: 0.0580     \n",
      "Epoch 10/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0223 - mean_squared_error: 0.0223 - acc: 0.0580     \n",
      "Epoch 11/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0192 - mean_squared_error: 0.0192 - acc: 0.0582     \n",
      "Epoch 12/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0188 - mean_squared_error: 0.0188 - acc: 0.0582     \n",
      "Epoch 13/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0172 - mean_squared_error: 0.0172 - acc: 0.0580     \n",
      "Epoch 14/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0163 - mean_squared_error: 0.0163 - acc: 0.0581     \n",
      "Epoch 15/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0149 - mean_squared_error: 0.0149 - acc: 0.0581     \n",
      "Epoch 16/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0149 - mean_squared_error: 0.0149 - acc: 0.0581     \n",
      "Epoch 17/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0141 - mean_squared_error: 0.0141 - acc: 0.0582     \n",
      "Epoch 18/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0127 - mean_squared_error: 0.0127 - acc: 0.0581     \n",
      "Epoch 19/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0129 - mean_squared_error: 0.0129 - acc: 0.0581     \n",
      "Epoch 20/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0110 - mean_squared_error: 0.0110 - acc: 0.0582     \n",
      "Epoch 21/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0124 - mean_squared_error: 0.0124 - acc: 0.0581     \n",
      "Epoch 22/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0126 - mean_squared_error: 0.0126 - acc: 0.0583     \n",
      "Epoch 23/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0100 - mean_squared_error: 0.0100 - acc: 0.0583     \n",
      "Epoch 24/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0110 - mean_squared_error: 0.0110 - acc: 0.0581     \n",
      "Epoch 25/25\n",
      "19824/19824 [==============================] - 0s - loss: 0.0098 - mean_squared_error: 0.0098 - acc: 0.0583     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faad1f84690>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_pairs[:, 0], data_pairs[:, 1]], labels, batch_size=24, verbose=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84655720338983054"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lambda x,y: [model.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_features_sequential(seq_data, track_data, len_segment, m_data, with_year=False, with_popularity=False):\n",
    "    \n",
    "    # sequential data\n",
    "    segments = seq_data[\"segments\"]\n",
    "    sl       = len(segments)\n",
    "    \n",
    "    mfcc              = np.array([s[\"timbre\"]            for s in segments])\n",
    "    chroma            = np.array([s[\"pitches\"]           for s in segments])\n",
    "    loudness_max      = np.array([s[\"loudness_max\"]      for s in segments]).reshape((sl,1))\n",
    "    loudness_start    = np.array([s[\"loudness_start\"]    for s in segments]).reshape((sl,1))\n",
    "    loudness_max_time = np.array([s[\"loudness_max_time\"] for s in segments]).reshape((sl,1))\n",
    "    duration          = np.array([s[\"duration\"]          for s in segments]).reshape((sl,1))\n",
    "    confidence        = np.array([s[\"confidence\"]        for s in segments]).reshape((sl,1))\n",
    "    \n",
    "    # concatenate sequential features\n",
    "    sequential_features = np.concatenate([mfcc, chroma, loudness_max, loudness_start, \n",
    "                                          loudness_max_time, duration, confidence], axis=1)\n",
    "    \n",
    "    offset  = np.random.randint(0, sl - len_segment)\n",
    "    segment = sequential_features[offset:(offset+len_segment),:]\n",
    "        \n",
    "    # track-based data\n",
    "    track_features = [track_data[0][\"acousticness\"],     # acoustic or not?\n",
    "                      track_data[0][\"danceability\"],     # danceable?\n",
    "                      track_data[0][\"energy\"],           # energetic or calm?\n",
    "                      track_data[0][\"instrumentalness\"], # is somebody singing?\n",
    "                      track_data[0][\"liveness\"],         # live or studio?\n",
    "                      track_data[0][\"speechiness\"],      # rap or singing?\n",
    "                      track_data[0][\"tempo\"],            # slow or fast?\n",
    "                      track_data[0][\"time_signature\"],   # 3/4, 4/4, 6/8, etc.\n",
    "                      track_data[0][\"valence\"]]          # happy or sad?\n",
    "    \n",
    "    if with_year:\n",
    "        track_features.append(int(m_data[\"year\"]))\n",
    "        \n",
    "    if with_popularity:\n",
    "        track_features.append(int(m_data[\"popularity\"]))\n",
    "        \n",
    "    \n",
    "    return segment, track_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_features.shape: (944, 20, 29)\n",
      "trackbased_features.shape: (944, 11)\n"
     ]
    }
   ],
   "source": [
    "len_segment = 20\n",
    "\n",
    "sequential_features = []\n",
    "trackbased_features = []\n",
    "\n",
    "for i, (_, spotify_data) in enumerate(raw_track_data):\n",
    "    \n",
    "    _, _, _, f_sequential, f_trackbased = spotify_data\n",
    "    \n",
    "    seq_feat, track_feat = aggregate_features_sequential(f_sequential, \n",
    "                                                         f_trackbased, \n",
    "                                                         len_segment, \n",
    "                                                         metadata.loc[i],\n",
    "                                                         with_year=True,\n",
    "                                                         with_popularity=True)\n",
    "    \n",
    "    sequential_features.append(seq_feat)\n",
    "    trackbased_features.append(track_feat)\n",
    "    \n",
    "sequential_features = np.asarray(sequential_features)\n",
    "trackbased_features = np.asarray(trackbased_features)\n",
    "\n",
    "print \"sequential_features.shape:\", sequential_features.shape\n",
    "print \"trackbased_features.shape:\", trackbased_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize sequential_features\n",
    "rows, x, y = sequential_features.shape\n",
    "sequential_features = sequential_features.reshape(rows, (x * y))\n",
    "sequential_features -= sequential_features.mean(axis=0)\n",
    "sequential_features /= sequential_features.std(axis=0)\n",
    "sequential_features = sequential_features.reshape(rows, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize trackbased_features\n",
    "trackbased_features -= trackbased_features.mean(axis=0)\n",
    "trackbased_features /= trackbased_features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pairs_with_sims_and_identity_segments(sequential_features, trackbased_features, metadata, num_pairs_per_track, playlist_similarities):\n",
    "    \n",
    "    data_pairs_seq   = []\n",
    "    data_pairs_track = []\n",
    "    labels           = []\n",
    "    \n",
    "    for row_id, q_track in metadata.sample(frac=1).iterrows():\n",
    "        \n",
    "        data_pairs_seq.append([sequential_features[[row_id]][0], sequential_features[[row_id]][0]])\n",
    "        data_pairs_track.append([trackbased_features[[row_id]][0], trackbased_features[[row_id]][0]])\n",
    "        labels.append(1)\n",
    "        \n",
    "        for _ in range(num_pairs_per_track):\n",
    "            \n",
    "            # search similar and dissimilar examples\n",
    "            pos_example = metadata[metadata.playlist == q_track.playlist].sample(1)\n",
    "            neg_example = metadata[metadata.playlist != q_track.playlist].sample(1)\n",
    "\n",
    "            # create feature pairs\n",
    "            data_pairs_seq.append([sequential_features[[row_id]][0], sequential_features[[pos_example.index]][0]])\n",
    "            data_pairs_track.append([trackbased_features[[row_id]][0], trackbased_features[[pos_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, pos_example.playlist] - 0.1)\n",
    "\n",
    "            data_pairs_seq.append([sequential_features[[row_id]][0], sequential_features[[neg_example.index]][0]])\n",
    "            data_pairs_track.append([trackbased_features[[row_id]][0], trackbased_features[[neg_example.index]][0]])\n",
    "            labels.append(playlist_similarities.loc[q_track.playlist, neg_example.playlist] - 0.1)\n",
    "\n",
    "    return np.array(data_pairs_seq), np.array(data_pairs_track), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs_seq, data_pairs_track, labels = create_pairs_with_sims_and_identity_segments(sequential_features,\n",
    "                                                                                        trackbased_features,\n",
    "                                                                                        metadata, \n",
    "                                                                                        10, \n",
    "                                                                                        playlist_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional, Input, Lambda\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda,Convolution1D\n",
    "from keras.optimizers import RMSprop, Nadam, SGD\n",
    "from keras.regularizers import l2, l1\n",
    "from keras import backend as K\n",
    "#from keras.constraint import unit_norm\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_pairs_seq[:, 0].shape[1:]\n",
    "\n",
    "input_a = Input(shape=data_pairs_seq[:, 0].shape[1:])\n",
    "input_b = Input(shape=data_pairs_seq[:, 0].shape[1:])\n",
    "input_a2 = Input(shape=data_pairs_track[:, 0].shape[1:])\n",
    "input_b2 = Input(shape=data_pairs_track[:, 0].shape[1:])\n",
    "\n",
    "bdlstm = Bidirectional(LSTM(29, return_sequences=False, activation=\"selu\"))\n",
    "\n",
    "processed_a = bdlstm(input_a)\n",
    "processed_b = bdlstm(input_b)\n",
    "\n",
    "dens = Dense(9, activation=\"selu\")\n",
    "\n",
    "processed_a2 = dens(input_a2)\n",
    "processed_b2 = dens(input_b2)\n",
    "\n",
    "left = concatenate([processed_a, processed_a2], axis=1)\n",
    "right = concatenate([processed_b, processed_b2], axis=1)\n",
    "\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "\n",
    "distance = Lambda(L1_distance,\n",
    "                  output_shape=lambda x: x[0])([left, right])\n",
    "\n",
    "prediction = Dense(29 + 9, activation=\"elu\")(distance)\n",
    "#prediction = Dense(64, activation=\"elu\")(prediction)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(prediction)\n",
    "\n",
    "model = Model([input_a, input_b, input_a2, input_b2], prediction)\n",
    "\n",
    "# train\n",
    "rms = Nadam(lr=0.001)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=rms, metrics=[\"mean_squared_error\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.1451 - mean_squared_error: 0.1451 - acc: 0.0522    \n",
      "Epoch 2/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.1023 - mean_squared_error: 0.1023 - acc: 0.0535    \n",
      "Epoch 3/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0857 - mean_squared_error: 0.0857 - acc: 0.0543    \n",
      "Epoch 4/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0712 - mean_squared_error: 0.0712 - acc: 0.0558    \n",
      "Epoch 5/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0595 - mean_squared_error: 0.0595 - acc: 0.0566    \n",
      "Epoch 6/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0511 - mean_squared_error: 0.0511 - acc: 0.0571    \n",
      "Epoch 7/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0446 - mean_squared_error: 0.0446 - acc: 0.0574    \n",
      "Epoch 8/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0386 - mean_squared_error: 0.0386 - acc: 0.0577    \n",
      "Epoch 9/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0348 - mean_squared_error: 0.0348 - acc: 0.0573    \n",
      "Epoch 10/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0295 - mean_squared_error: 0.0295 - acc: 0.0577    \n",
      "Epoch 11/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0257 - mean_squared_error: 0.0257 - acc: 0.0578    \n",
      "Epoch 12/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0229 - mean_squared_error: 0.0229 - acc: 0.0577    \n",
      "Epoch 13/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0204 - mean_squared_error: 0.0204 - acc: 0.0578    \n",
      "Epoch 14/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0191 - mean_squared_error: 0.0191 - acc: 0.0578    \n",
      "Epoch 15/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0166 - mean_squared_error: 0.0166 - acc: 0.0578    \n",
      "Epoch 16/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0159 - mean_squared_error: 0.0159 - acc: 0.0578    \n",
      "Epoch 17/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0143 - mean_squared_error: 0.0143 - acc: 0.0579    \n",
      "Epoch 18/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0126 - mean_squared_error: 0.0126 - acc: 0.0579    \n",
      "Epoch 19/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0148 - mean_squared_error: 0.0148 - acc: 0.0579    \n",
      "Epoch 20/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0127 - mean_squared_error: 0.0127 - acc: 0.0579    \n",
      "Epoch 21/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0107 - mean_squared_error: 0.0107 - acc: 0.0579    \n",
      "Epoch 22/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0109 - mean_squared_error: 0.0109 - acc: 0.0579    \n",
      "Epoch 23/25\n",
      "19824/19824 [==============================] - 19s - loss: 0.0101 - mean_squared_error: 0.0101 - acc: 0.0579    \n",
      "Epoch 24/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0103 - mean_squared_error: 0.0103 - acc: 0.0579    \n",
      "Epoch 25/25\n",
      "19824/19824 [==============================] - 20s - loss: 0.0092 - mean_squared_error: 0.0092 - acc: 0.0579    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faac1d3c250>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_pairs_seq[:, 0], data_pairs_seq[:, 1], data_pairs_track[:,0], data_pairs_track[:,1]], labels, batch_size=24, verbose=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(similarity_function, cut_off):\n",
    "\n",
    "    all_precisions = []\n",
    "    \n",
    "    pbar = progressbar.ProgressBar()\n",
    "\n",
    "    for idx in pbar(metadata.index.values):\n",
    "\n",
    "        dist           = similarity_function(sequential_features, sequential_features[[idx]], trackbased_features, trackbased_features[[idx]])\n",
    "        dist           = np.array(dist).reshape(len(dist))\n",
    "        similar_tracks = metadata.loc[np.argsort(dist)[::-1][:cut_off]]\n",
    "        same_label     = similar_tracks[\"playlist\"] == metadata.loc[idx, \"playlist\"]\n",
    "        precision      = same_label.sum() / float(cut_off)\n",
    "        all_precisions.append(precision)\n",
    "\n",
    "    all_precisions = np.array(all_precisions)\n",
    "\n",
    "    return all_precisions.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (944 of 944) |#####################################################################################################################################################################################| Elapsed Time: 1:20:00 Time: 1:20:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85317796610169494"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lambda w,x,y,z: [model.predict([w[[i]],x,y[[i]],z]) for i in range(sequential_features.shape[0])], 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "5",
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
